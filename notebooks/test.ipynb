{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd0318d1cc9f84053ca3aef420ad2adb5bdb5c3a681788cc08f605fed222c2916a2",
   "display_name": "Python 3.6.13 64-bit ('gvp': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "318d1cc9f84053ca3aef420ad2adb5bdb5c3a681788cc08f605fed222c2916a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_geometric.nn import knn_graph\n",
    "from torch_geometric import transforms\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "from geometric_vector_perceptron import GVP_Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/ysk2a15/mydocuments/EEE_Y4/COMP6248/Geometric-Vector-Perceptron\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(os.getcwd()+\"/gvp/data/synthetic\")\n",
    "\n",
    "cnn = torch.from_numpy(np.load(data_path/\"cnn.npy\"))\n",
    "synthetic = torch.from_numpy(np.load(data_path/\"synthetic.npy\"))\n",
    "with np.load(data_path/\"answers.npz\") as data:\n",
    "    # off_center = torch.from_numpy(data[\"off_center\"])\n",
    "    perimeter = torch.from_numpy(data[\"perimeter\"])\n",
    "off_center = torch.from_numpy(np.load(data_path/\"OCR.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20000, 2, 100, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "synthetic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(8.5405)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "max(off_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_matrix(vectors):\n",
    "    b, _, d = vectors.shape\n",
    "    # Adding new axis with None\n",
    "    diff = vectors[..., None, :] - vectors[:, None, ...]\n",
    "    return diff.reshape(b, -1, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 32, 3])\ntorch.Size([1, 32, 1, 3])\ntorch.Size([1, 1, 32, 3])\ntorch.Size([1, 32, 32, 3])\ntorch.Size([1, 1024, 3])\n"
     ]
    }
   ],
   "source": [
    "vectors = torch.randn(1, 32, 3)\n",
    "print(vectors.shape)\n",
    "\n",
    "print(vectors[..., None, :].shape)\n",
    "print(vectors[:, None, ...].shape)\n",
    "\n",
    "a = vectors[..., None, :]\n",
    "b = vectors[:, None, ...] \n",
    "\n",
    "diff = a-b \n",
    "print(diff.shape)\n",
    "\n",
    "print(diff.reshape(diff.shape[0], diff.shape[1]*diff.shape[2], diff.shape[3]).shape)\n",
    "\n",
    "\n",
    "# tmp = diff_matrix(vectors)\n",
    "# print(tmp.shape)\n",
    "# print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "h = max(1024, 256)\n",
    "h"
   ]
  },
  {
   "source": [
    "## Check dims of each layer of GVP model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.7505, -0.5612,  0.4169,  ..., -1.2299, -1.6742,  1.1036],\n",
       "        [-2.7312,  1.3213,  0.9751,  ...,  1.0150, -0.3535, -1.4883],\n",
       "        [-0.6762,  0.7622, -0.4388,  ...,  0.5518, -0.8311,  0.8096],\n",
       "        ...,\n",
       "        [-1.0033,  1.2610, -0.6265,  ...,  1.0308, -0.4087, -0.9085],\n",
       "        [-1.6114, -0.0151, -0.1247,  ...,  0.2509, -0.0923, -1.4548],\n",
       "        [-0.9851, -1.1293, -0.2806,  ..., -1.0230,  0.8629, -0.2491]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "dim_vectors_in = 1024\n",
    "dim_vectors_out = 256\n",
    "dim_h = h\n",
    "\n",
    "Wh = nn.Parameter(torch.randn(dim_vectors_in, dim_h))\n",
    "Wu = nn.Parameter(torch.randn(dim_h, dim_vectors_out))\n",
    "\n",
    "Wu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 512 1 32 3\n"
     ]
    }
   ],
   "source": [
    "feats = torch.randn(1, 512)\n",
    "\n",
    "b, n, _, v, c  = *feats.shape, *vectors.shape\n",
    "\n",
    "print(b, n, _, v, c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "diff_matrix(vectors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# this is equivalent to torch.unsqueeze(a.mm(b), 0)\n",
    "# keep first dim \n",
    "\n",
    "Vh = einsum('b v c, v h -> b h c', diff_matrix(vectors), Wh)\n",
    "Vu = einsum('b h c, h u -> b u c', Vh, Wu)\n",
    "\n",
    "Vh.shape\n",
    "Vu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "sh = torch.norm(Vh, p = 2, dim = -1)\n",
    "vu = torch.norm(Vu, p = 2, dim = -1, keepdim = True)\n",
    "\n",
    "sh.shape\n",
    "vu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1536])"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "s = torch.cat((feats, sh), dim = 1)\n",
    "\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_feats_in = 512\n",
    "dim_feats_out = 512\n",
    "feats_activation = vectors_activation = nn.Sigmoid()\n",
    "\n",
    "to_feats_out = nn.Sequential(\n",
    "            nn.Linear(dim_h + dim_feats_in, dim_feats_out),\n",
    "            feats_activation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "feats_out = to_feats_out(s)\n",
    "feats_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "vectors_out = vectors_activation(vu) * Vu\n",
    "\n",
    "vectors_out.shape"
   ]
  },
  {
   "source": [
    "## Check for Equivariance and Invariance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation():\n",
    "    # Compute QR decomposition, q=orthogonal matrix\n",
    "    # transformation described as multiplying V (vectors) with unitary matrix [@paper]\n",
    "    # unitary matirx = orthogonal matrix (3,3)\n",
    "    q, r = torch.qr(torch.randn(3, 3))\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometric_vector_perceptron import GVP\n",
    "\n",
    "R = random_rotation()\n",
    "\n",
    "model = GVP(\n",
    "        dim_vectors_in = 1024,\n",
    "        dim_feats_in = 512,\n",
    "        dim_vectors_out = 256,\n",
    "        dim_feats_out = 512\n",
    "    )\n",
    "\n",
    "\n",
    "feats_out, vectors_out = model( (feats, diff_matrix(vectors)) )\n",
    "feats_out_r, vectors_out_r = model( (feats, diff_matrix(vectors @ R)) )\n"
   ]
  },
  {
   "source": [
    "### Features (scalar) invariance w.r.t. rotations and reflections"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0000e+00, 2.5277e-12, 8.6966e-13, 8.0424e-03, 1.0000e+00,\n",
       "        7.3293e-05, 8.4491e-26, 5.4068e-15, 1.0000e+00, 5.5394e-18, 1.0000e+00,\n",
       "        1.0000e+00, 9.9999e-01, 1.0000e+00, 8.3762e-11, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 4.9738e-06, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9440e-01,\n",
       "        1.8495e-01, 1.0000e+00, 7.1932e-08, 9.6137e-13, 7.0642e-22, 4.5547e-14,\n",
       "        4.3304e-05, 1.0000e+00, 1.0000e+00, 4.4329e-26, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 9.9999e-01, 1.0000e+00, 3.8878e-07, 2.0826e-11, 8.4437e-01,\n",
       "        1.0000e+00, 1.0000e+00, 9.9995e-01, 1.0702e-21, 7.1420e-05, 9.4205e-18,\n",
       "        2.8663e-08, 3.3247e-34, 4.2385e-02, 9.8943e-01, 2.3855e-07, 3.3991e-13,\n",
       "        1.0000e+00, 9.2004e-01, 1.8622e-10, 3.9961e-02, 9.6349e-02, 2.6811e-08,\n",
       "        1.2512e-12, 3.4125e-23, 8.2422e-25, 2.6073e-11, 9.9969e-01, 1.0000e+00,\n",
       "        1.0000e+00, 4.5652e-24, 1.0000e+00, 9.9999e-01, 1.0000e+00, 8.9313e-01,\n",
       "        9.9779e-01, 9.7619e-01, 9.9999e-01, 1.0000e+00, 3.0185e-02, 1.2284e-03,\n",
       "        1.0000e+00, 1.0000e+00, 6.2457e-06, 9.9842e-01, 1.0000e+00, 1.0000e+00,\n",
       "        7.8561e-01, 1.0000e+00, 6.9305e-19, 1.0000e+00, 8.4792e-01, 1.1235e-29,\n",
       "        1.0000e+00, 9.9912e-01, 1.4463e-33, 1.0000e+00, 1.0000e+00, 5.3762e-29,\n",
       "        6.7776e-04, 1.0000e+00, 1.0000e+00, 1.2424e-14, 9.9979e-01, 7.0802e-01,\n",
       "        1.7638e-05, 9.9999e-01, 3.3402e-03, 1.0000e+00, 3.5196e-13, 1.0000e+00,\n",
       "        9.9990e-01, 9.1974e-14, 9.9999e-01, 8.0785e-17, 1.0000e+00, 1.0000e+00,\n",
       "        5.3405e-10, 2.3547e-26, 1.5210e-20, 1.0000e+00, 3.1240e-01, 5.5895e-09,\n",
       "        1.0000e+00, 1.0879e-32, 1.0000e+00, 7.1231e-01, 1.0000e+00, 1.0000e+00,\n",
       "        9.9782e-01, 1.0000e+00, 2.5880e-15, 1.0000e+00, 1.0000e+00, 1.7880e-13,\n",
       "        8.5378e-21, 8.6335e-11, 1.0000e+00, 5.8669e-01, 1.0000e+00, 2.2511e-09,\n",
       "        1.0000e+00, 1.0000e+00, 2.6548e-02, 1.4521e-06, 1.0000e+00, 1.0000e+00,\n",
       "        7.1493e-13, 3.5176e-13, 1.0000e+00, 1.0000e+00, 5.3815e-01, 2.5132e-15,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 9.6061e-01, 1.0000e+00, 9.9976e-01,\n",
       "        2.9457e-02, 1.0000e+00, 3.4473e-15, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.6640e-02, 4.2908e-09, 4.4701e-02, 9.9909e-01, 5.8118e-01, 9.9912e-01,\n",
       "        7.3177e-32, 1.1365e-19, 0.0000e+00, 9.9999e-01, 3.2924e-28, 9.0171e-04,\n",
       "        9.4758e-08, 2.7743e-05, 1.9074e-07, 5.0805e-15, 7.2898e-11, 1.0000e+00,\n",
       "        1.0000e+00, 4.1666e-13, 9.9956e-01, 9.9526e-01, 2.6711e-05, 9.9829e-01,\n",
       "        0.0000e+00, 1.9468e-08, 1.6244e-03, 1.0000e+00, 1.3681e-14, 1.0000e+00,\n",
       "        1.0000e+00, 3.4819e-14, 1.0000e+00, 1.0000e+00, 1.0000e+00, 8.3248e-05,\n",
       "        1.0000e+00, 2.9021e-23, 1.0000e+00, 1.0000e+00, 2.0325e-24, 2.6361e-01,\n",
       "        1.0000e+00, 4.9094e-12, 1.0000e+00, 4.0262e-02, 1.5092e-11, 1.0000e+00,\n",
       "        6.9098e-12, 1.0000e+00, 6.1653e-02, 1.1807e-05, 3.0173e-09, 1.5100e-05,\n",
       "        1.0000e+00, 2.0643e-01, 4.4438e-06, 1.0000e+00, 3.4692e-25, 1.0000e+00,\n",
       "        1.2876e-21, 1.7754e-21, 1.0000e+00, 1.0000e+00, 9.3852e-03, 9.9949e-01,\n",
       "        1.0000e+00, 1.0000e+00, 1.5870e-04, 5.5735e-06, 1.9162e-13, 3.3829e-06,\n",
       "        2.3668e-11, 1.0000e+00, 9.1720e-01, 1.0000e+00, 1.0000e+00, 9.1782e-01,\n",
       "        1.0000e+00, 1.1493e-01, 1.3708e-16, 1.0000e+00, 1.0000e+00, 4.2634e-19,\n",
       "        9.4847e-19, 2.7762e-10, 2.5253e-08, 9.9949e-01, 5.1811e-22, 1.7974e-21,\n",
       "        3.5679e-03, 1.0000e+00, 1.2132e-03, 1.0000e+00, 1.0000e+00, 6.5292e-09,\n",
       "        1.2762e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.1108e-01, 5.2625e-26,\n",
       "        1.0000e+00, 6.5071e-06, 1.0000e+00, 1.0000e+00, 1.3086e-31, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 9.6536e-01, 1.0000e+00, 1.0000e+00, 2.4712e-02,\n",
       "        5.8893e-01, 1.0000e+00, 3.6257e-27, 6.5455e-01, 1.3024e-06, 8.9819e-35,\n",
       "        5.1960e-22, 1.0000e+00, 1.8091e-38, 9.8447e-11, 3.6329e-13, 1.0000e+00,\n",
       "        7.1931e-16, 1.0000e+00, 1.0000e+00, 1.7562e-01, 1.0000e+00, 5.8732e-01,\n",
       "        1.0000e+00, 5.0646e-01, 8.2725e-01, 6.7480e-36, 1.0000e+00, 5.9669e-15,\n",
       "        9.8955e-01, 1.2601e-07, 9.9244e-01, 9.9999e-01, 4.7763e-01, 3.7757e-02,\n",
       "        1.0000e+00, 1.0000e+00, 3.3936e-02, 2.6318e-02, 7.6955e-08, 1.0000e+00,\n",
       "        4.3861e-30, 9.9985e-01, 1.4052e-14, 6.5762e-19, 1.1283e-36, 1.3638e-24,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9997e-01, 1.0000e+00, 9.9994e-01,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.3860e-02, 5.5110e-08, 2.5468e-17,\n",
       "        1.0000e+00, 1.0000e+00, 1.0708e-15, 5.2482e-24, 1.0000e+00, 1.0000e+00,\n",
       "        9.9999e-01, 8.9143e-01, 1.0611e-29, 1.0000e+00, 1.0000e+00, 3.3054e-06,\n",
       "        1.0000e+00, 1.0000e+00, 5.2319e-12, 1.8924e-04, 2.2005e-27, 1.6954e-24,\n",
       "        1.3744e-25, 5.1857e-08, 9.9020e-01, 1.0000e+00, 1.0000e+00, 1.2254e-01,\n",
       "        1.0000e+00, 6.8473e-14, 8.7281e-12, 1.0000e+00, 1.0000e+00, 1.8464e-08,\n",
       "        6.1356e-07, 1.0000e+00, 1.9881e-14, 1.1073e-17, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 2.9843e-20, 1.0000e+00, 9.9994e-01, 9.9997e-01,\n",
       "        1.9848e-19, 6.4663e-17, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.8717e-18,\n",
       "        2.7203e-08, 1.0000e+00, 2.0300e-29, 1.0000e+00, 9.2611e-02, 1.1778e-08,\n",
       "        5.7478e-11, 1.0000e+00, 1.3261e-11, 1.0000e+00, 1.0000e+00, 6.1228e-01,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 8.8232e-01, 1.0000e+00, 6.1616e-03,\n",
       "        5.8096e-09, 1.0000e+00, 1.0000e+00, 1.8675e-13, 8.9793e-01, 1.0000e+00,\n",
       "        4.7826e-19, 5.5858e-01, 1.0000e+00, 1.0000e+00, 5.3259e-17, 9.2443e-14,\n",
       "        4.7009e-02, 5.5706e-02, 4.7279e-07, 1.0000e+00, 1.1692e-11, 1.0000e+00,\n",
       "        1.7214e-04, 1.8563e-10, 1.0000e+00, 1.7842e-19, 1.0000e+00, 1.0000e+00,\n",
       "        5.8762e-10, 1.0000e+00, 3.0680e-07, 1.5848e-01, 9.9974e-01, 2.5635e-19,\n",
       "        1.0000e+00, 5.5425e-37, 2.5440e-10, 6.2382e-05, 1.0000e+00, 7.3216e-08,\n",
       "        1.0000e+00, 1.0422e-06, 7.0886e-01, 7.7850e-07, 1.0000e+00, 3.5030e-08,\n",
       "        1.0000e+00, 2.2493e-24, 1.1911e-10, 1.5500e-08, 9.9995e-01, 1.0000e+00,\n",
       "        3.1441e-04, 2.9904e-11, 3.6868e-05, 2.9344e-15, 6.9231e-02, 2.1004e-21,\n",
       "        9.9998e-01, 1.0000e+00, 9.9999e-01, 1.0000e+00, 9.9883e-01, 9.7169e-01,\n",
       "        9.9970e-01, 8.5494e-36, 9.9317e-01, 1.9827e-12, 1.3264e-29, 1.0000e+00,\n",
       "        9.9853e-01, 1.6746e-05, 1.0000e+00, 1.7679e-17, 1.0000e+00, 1.0000e+00,\n",
       "        6.3103e-18, 1.0000e+00, 4.6496e-09, 1.0000e+00, 2.4797e-14, 9.9991e-01,\n",
       "        1.5967e-07, 9.1458e-38, 9.1233e-28, 1.0000e+00, 2.2201e-18, 1.4357e-11,\n",
       "        1.0000e+00, 4.6157e-03, 9.9994e-01, 1.0000e+00, 1.0000e+00, 8.1639e-01,\n",
       "        4.9806e-12, 8.4133e-03, 8.1984e-21, 9.9999e-01, 1.0000e+00, 1.0000e+00,\n",
       "        3.8407e-10, 1.0000e+00, 5.4745e-05, 1.8106e-05, 9.3739e-01, 1.0000e+00,\n",
       "        1.0928e-09, 5.2432e-05, 1.8684e-26, 1.2962e-03, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 4.3871e-19, 9.9998e-01, 5.8666e-05,\n",
       "        1.0000e+00, 1.0000e+00], grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "feats_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0000e+00, 2.5277e-12, 8.6966e-13, 8.0425e-03, 1.0000e+00,\n",
       "        7.3292e-05, 8.4490e-26, 5.4068e-15, 1.0000e+00, 5.5394e-18, 1.0000e+00,\n",
       "        1.0000e+00, 9.9999e-01, 1.0000e+00, 8.3762e-11, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 4.9738e-06, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9440e-01,\n",
       "        1.8495e-01, 1.0000e+00, 7.1932e-08, 9.6137e-13, 7.0641e-22, 4.5547e-14,\n",
       "        4.3304e-05, 1.0000e+00, 1.0000e+00, 4.4329e-26, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 9.9999e-01, 1.0000e+00, 3.8878e-07, 2.0826e-11, 8.4437e-01,\n",
       "        1.0000e+00, 1.0000e+00, 9.9995e-01, 1.0702e-21, 7.1420e-05, 9.4206e-18,\n",
       "        2.8664e-08, 3.3248e-34, 4.2384e-02, 9.8943e-01, 2.3855e-07, 3.3991e-13,\n",
       "        1.0000e+00, 9.2004e-01, 1.8622e-10, 3.9961e-02, 9.6350e-02, 2.6811e-08,\n",
       "        1.2512e-12, 3.4125e-23, 8.2422e-25, 2.6073e-11, 9.9969e-01, 1.0000e+00,\n",
       "        1.0000e+00, 4.5651e-24, 1.0000e+00, 9.9999e-01, 1.0000e+00, 8.9313e-01,\n",
       "        9.9779e-01, 9.7619e-01, 9.9999e-01, 1.0000e+00, 3.0185e-02, 1.2284e-03,\n",
       "        1.0000e+00, 1.0000e+00, 6.2457e-06, 9.9842e-01, 1.0000e+00, 1.0000e+00,\n",
       "        7.8561e-01, 1.0000e+00, 6.9304e-19, 1.0000e+00, 8.4792e-01, 1.1236e-29,\n",
       "        1.0000e+00, 9.9912e-01, 1.4463e-33, 1.0000e+00, 1.0000e+00, 5.3761e-29,\n",
       "        6.7777e-04, 1.0000e+00, 1.0000e+00, 1.2424e-14, 9.9979e-01, 7.0802e-01,\n",
       "        1.7638e-05, 9.9999e-01, 3.3402e-03, 1.0000e+00, 3.5196e-13, 1.0000e+00,\n",
       "        9.9990e-01, 9.1975e-14, 9.9999e-01, 8.0784e-17, 1.0000e+00, 1.0000e+00,\n",
       "        5.3405e-10, 2.3547e-26, 1.5209e-20, 1.0000e+00, 3.1240e-01, 5.5894e-09,\n",
       "        1.0000e+00, 1.0879e-32, 1.0000e+00, 7.1231e-01, 1.0000e+00, 1.0000e+00,\n",
       "        9.9782e-01, 1.0000e+00, 2.5879e-15, 1.0000e+00, 1.0000e+00, 1.7880e-13,\n",
       "        8.5377e-21, 8.6335e-11, 1.0000e+00, 5.8669e-01, 1.0000e+00, 2.2511e-09,\n",
       "        1.0000e+00, 1.0000e+00, 2.6548e-02, 1.4521e-06, 1.0000e+00, 1.0000e+00,\n",
       "        7.1495e-13, 3.5176e-13, 1.0000e+00, 1.0000e+00, 5.3814e-01, 2.5132e-15,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 9.6061e-01, 1.0000e+00, 9.9976e-01,\n",
       "        2.9457e-02, 1.0000e+00, 3.4473e-15, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.6640e-02, 4.2907e-09, 4.4701e-02, 9.9909e-01, 5.8118e-01, 9.9912e-01,\n",
       "        7.3177e-32, 1.1365e-19, 0.0000e+00, 9.9999e-01, 3.2924e-28, 9.0170e-04,\n",
       "        9.4758e-08, 2.7743e-05, 1.9074e-07, 5.0805e-15, 7.2898e-11, 1.0000e+00,\n",
       "        1.0000e+00, 4.1666e-13, 9.9956e-01, 9.9526e-01, 2.6711e-05, 9.9829e-01,\n",
       "        0.0000e+00, 1.9468e-08, 1.6244e-03, 1.0000e+00, 1.3681e-14, 1.0000e+00,\n",
       "        1.0000e+00, 3.4819e-14, 1.0000e+00, 1.0000e+00, 1.0000e+00, 8.3248e-05,\n",
       "        1.0000e+00, 2.9021e-23, 1.0000e+00, 1.0000e+00, 2.0325e-24, 2.6361e-01,\n",
       "        1.0000e+00, 4.9095e-12, 1.0000e+00, 4.0262e-02, 1.5092e-11, 1.0000e+00,\n",
       "        6.9098e-12, 1.0000e+00, 6.1653e-02, 1.1807e-05, 3.0173e-09, 1.5101e-05,\n",
       "        1.0000e+00, 2.0643e-01, 4.4438e-06, 1.0000e+00, 3.4692e-25, 1.0000e+00,\n",
       "        1.2876e-21, 1.7754e-21, 1.0000e+00, 1.0000e+00, 9.3852e-03, 9.9949e-01,\n",
       "        1.0000e+00, 1.0000e+00, 1.5870e-04, 5.5735e-06, 1.9162e-13, 3.3829e-06,\n",
       "        2.3668e-11, 1.0000e+00, 9.1720e-01, 1.0000e+00, 1.0000e+00, 9.1782e-01,\n",
       "        1.0000e+00, 1.1493e-01, 1.3708e-16, 1.0000e+00, 1.0000e+00, 4.2633e-19,\n",
       "        9.4847e-19, 2.7762e-10, 2.5253e-08, 9.9949e-01, 5.1811e-22, 1.7974e-21,\n",
       "        3.5678e-03, 1.0000e+00, 1.2132e-03, 1.0000e+00, 1.0000e+00, 6.5291e-09,\n",
       "        1.2762e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.1108e-01, 5.2624e-26,\n",
       "        1.0000e+00, 6.5071e-06, 1.0000e+00, 1.0000e+00, 1.3086e-31, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 9.6536e-01, 1.0000e+00, 1.0000e+00, 2.4712e-02,\n",
       "        5.8893e-01, 1.0000e+00, 3.6256e-27, 6.5455e-01, 1.3024e-06, 8.9819e-35,\n",
       "        5.1960e-22, 1.0000e+00, 1.8091e-38, 9.8447e-11, 3.6329e-13, 1.0000e+00,\n",
       "        7.1931e-16, 1.0000e+00, 1.0000e+00, 1.7562e-01, 1.0000e+00, 5.8732e-01,\n",
       "        1.0000e+00, 5.0646e-01, 8.2725e-01, 6.7480e-36, 1.0000e+00, 5.9669e-15,\n",
       "        9.8955e-01, 1.2601e-07, 9.9244e-01, 9.9999e-01, 4.7763e-01, 3.7757e-02,\n",
       "        1.0000e+00, 1.0000e+00, 3.3936e-02, 2.6318e-02, 7.6956e-08, 1.0000e+00,\n",
       "        4.3861e-30, 9.9985e-01, 1.4052e-14, 6.5762e-19, 1.1283e-36, 1.3638e-24,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9997e-01, 1.0000e+00, 9.9994e-01,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.3860e-02, 5.5110e-08, 2.5468e-17,\n",
       "        1.0000e+00, 1.0000e+00, 1.0708e-15, 5.2481e-24, 1.0000e+00, 1.0000e+00,\n",
       "        9.9999e-01, 8.9143e-01, 1.0612e-29, 1.0000e+00, 1.0000e+00, 3.3054e-06,\n",
       "        1.0000e+00, 1.0000e+00, 5.2319e-12, 1.8924e-04, 2.2005e-27, 1.6954e-24,\n",
       "        1.3744e-25, 5.1858e-08, 9.9020e-01, 1.0000e+00, 1.0000e+00, 1.2254e-01,\n",
       "        1.0000e+00, 6.8473e-14, 8.7282e-12, 1.0000e+00, 1.0000e+00, 1.8464e-08,\n",
       "        6.1356e-07, 1.0000e+00, 1.9881e-14, 1.1073e-17, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 2.9843e-20, 1.0000e+00, 9.9994e-01, 9.9997e-01,\n",
       "        1.9848e-19, 6.4664e-17, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.8716e-18,\n",
       "        2.7203e-08, 1.0000e+00, 2.0300e-29, 1.0000e+00, 9.2611e-02, 1.1778e-08,\n",
       "        5.7479e-11, 1.0000e+00, 1.3261e-11, 1.0000e+00, 1.0000e+00, 6.1227e-01,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 8.8232e-01, 1.0000e+00, 6.1617e-03,\n",
       "        5.8096e-09, 1.0000e+00, 1.0000e+00, 1.8675e-13, 8.9793e-01, 1.0000e+00,\n",
       "        4.7826e-19, 5.5858e-01, 1.0000e+00, 1.0000e+00, 5.3259e-17, 9.2443e-14,\n",
       "        4.7009e-02, 5.5706e-02, 4.7279e-07, 1.0000e+00, 1.1693e-11, 1.0000e+00,\n",
       "        1.7214e-04, 1.8563e-10, 1.0000e+00, 1.7842e-19, 1.0000e+00, 1.0000e+00,\n",
       "        5.8762e-10, 1.0000e+00, 3.0680e-07, 1.5848e-01, 9.9974e-01, 2.5635e-19,\n",
       "        1.0000e+00, 5.5426e-37, 2.5440e-10, 6.2381e-05, 1.0000e+00, 7.3216e-08,\n",
       "        1.0000e+00, 1.0422e-06, 7.0886e-01, 7.7849e-07, 1.0000e+00, 3.5030e-08,\n",
       "        1.0000e+00, 2.2494e-24, 1.1911e-10, 1.5500e-08, 9.9995e-01, 1.0000e+00,\n",
       "        3.1441e-04, 2.9904e-11, 3.6868e-05, 2.9344e-15, 6.9231e-02, 2.1004e-21,\n",
       "        9.9998e-01, 1.0000e+00, 9.9999e-01, 1.0000e+00, 9.9883e-01, 9.7169e-01,\n",
       "        9.9970e-01, 8.5493e-36, 9.9317e-01, 1.9828e-12, 1.3264e-29, 1.0000e+00,\n",
       "        9.9853e-01, 1.6746e-05, 1.0000e+00, 1.7679e-17, 1.0000e+00, 1.0000e+00,\n",
       "        6.3103e-18, 1.0000e+00, 4.6496e-09, 1.0000e+00, 2.4797e-14, 9.9991e-01,\n",
       "        1.5967e-07, 9.1458e-38, 9.1232e-28, 1.0000e+00, 2.2202e-18, 1.4357e-11,\n",
       "        1.0000e+00, 4.6158e-03, 9.9994e-01, 1.0000e+00, 1.0000e+00, 8.1639e-01,\n",
       "        4.9806e-12, 8.4133e-03, 8.1984e-21, 9.9999e-01, 1.0000e+00, 1.0000e+00,\n",
       "        3.8407e-10, 1.0000e+00, 5.4745e-05, 1.8106e-05, 9.3740e-01, 1.0000e+00,\n",
       "        1.0928e-09, 5.2432e-05, 1.8684e-26, 1.2962e-03, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 4.3871e-19, 9.9998e-01, 5.8666e-05,\n",
       "        1.0000e+00, 1.0000e+00], grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "feats_out_r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True])"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "torch.eq(torch.round(feats_out[0]), torch.round(feats_out_r[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "torch.all(torch.eq(torch.round(feats_out[0]), torch.round(feats_out_r[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.8610e-06, grad_fn=<MaxBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "(feats_out - feats_out_r).max()"
   ]
  },
  {
   "source": [
    "### Vectors equivariance w.r.t. rotations and reflections"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[25183.6230, 21765.9180, 18937.5078]], grad_fn=<SvdHelperBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "u, s, v = torch.svd(vectors_out)\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[25183.6211, 21765.9180, 18937.5098]], grad_fn=<SvdHelperBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "u, s, v = torch.svd(vectors_out_r)\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[  211.3972,   507.6278,  1003.5482],\n",
       "        [ -569.8422,  -625.5551,   349.9399],\n",
       "        [ 2238.3110, -1697.3890,  1406.7788],\n",
       "        [ -544.5129, -1574.9205,  -700.5328],\n",
       "        [-2667.7134,  -123.2306, -1287.6260],\n",
       "        [ 1019.3002,  -937.6866,  2115.3357],\n",
       "        [ -258.0677,  1336.5889,  1083.5842],\n",
       "        [ -917.5706, -1021.5850,  1085.4001],\n",
       "        [ -988.6802, -1194.0969, -2335.5066],\n",
       "        [  880.2687,  1403.6772,  1974.6440]], grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "vectors_out[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 6.1974e+02, -8.3118e+02, -4.8430e+02],\n",
       "        [ 7.0712e+02,  3.7021e+02,  4.4881e+02],\n",
       "        [ 2.4127e+02, -2.2940e+03,  2.1329e+03],\n",
       "        [-1.4718e+00,  1.1169e+03,  1.4213e+03],\n",
       "        [ 4.1144e+02,  2.8962e+03, -4.8228e+02],\n",
       "        [ 1.3501e+03, -1.8540e+03,  1.0643e+03],\n",
       "        [ 7.9357e+02, -6.4784e+02, -1.4063e+03],\n",
       "        [ 1.5698e+03,  3.0181e+02,  7.1300e+02],\n",
       "        [-1.1706e+03,  2.3354e+03,  1.0166e+03],\n",
       "        [ 8.9600e+02, -2.0806e+03, -1.2300e+03]], grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "vectors_out_r[0][:10]"
   ]
  },
  {
   "source": [
    "- different values for with and without rotations --> equivariance \n",
    "- equivariance = if input changes, output changes \n",
    "- f(g(x)) = g(f(x))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}