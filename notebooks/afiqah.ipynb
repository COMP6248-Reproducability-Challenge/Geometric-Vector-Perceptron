{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0554b53c503a49b6068cbfae1b06e18ba29b53f31bb070bcb124be24e905a1b6a",
   "display_name": "Python 3.7.10 64-bit ('gvp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch_scatter\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"../data/synthetic\")\n",
    "with np.load(path/\"answers.npz\") as f:\n",
    "    ocr = torch.Tensor(f[\"off_center\"])\n",
    "    prm = torch.Tensor(f[\"perimeter\"])\n",
    "cnn = torch.Tensor( np.load(path/\"cnn.npy\") )\n",
    "syn = torch.Tensor( np.load(path/\"synthetic.npy\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(13.7476)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "prm.min()"
   ]
  },
  {
   "source": [
    "# Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(y):\n",
    "    min_y = y.min() #-10.\n",
    "    max_y = y.max() #10.\n",
    "    return ( y - min_y ) / ( max_y - min_y )\n",
    "\n",
    "class CNNDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir=\"../data/synthetic\", task=0, num_workers=8):\n",
    "        # 0: off-center, 1: perimeter 2: combined\n",
    "        super().__init__()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.task = task\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        X = torch.Tensor( np.load(self.data_dir/\"cnn.npy\") ).permute(0,4,1,2,3)\n",
    "        # X = X[:, ::2] # skip channel 1 - non-special atoms\n",
    "        \n",
    "        y = []\n",
    "        with np.load(self.data_dir/\"answers.npz\") as f:\n",
    "            # y.append( scaling(torch.Tensor(f[\"off_center\"])) )\n",
    "            # y.append( scaling(torch.Tensor(f[\"perimeter\"])) )\n",
    "            y.append( torch.Tensor(f[\"off_center\"]) )\n",
    "            y.append( torch.Tensor(f[\"perimeter\"]) )\n",
    "        # y.append( torch.abs(y[0]-y[1]) )\n",
    "        y.append( torch.abs(scaling(y[0]) - scaling(y[1])) )\n",
    "\n",
    "        dataset = TensorDataset(X, y[self.task])\n",
    "\n",
    "        # Split\n",
    "        full, test = random_split(dataset, [18000, 2000])\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.cnn_train, self.cnn_val = random_split(full, [16000, 2000])\n",
    "            self.dims = tuple(self.cnn_train[0][0].shape)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.cnn_test = test\n",
    "            self.dims = tuple(self.cnn_test[0][0].shape)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.cnn_train, batch_size=32, num_workers=self.num_workers)\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.cnn_val, batch_size=32, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.cnn_test, batch_size=32, num_workers=self.num_workers)"
   ]
  },
  {
   "source": [
    "# CNN model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "58017"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "KERNEL_SIZE = 3\n",
    "\n",
    "class ShallowCNN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(3, 32, (KERNEL_SIZE,KERNEL_SIZE,KERNEL_SIZE))\n",
    "        self.conv2 = nn.Conv3d(32, 32, (KERNEL_SIZE,KERNEL_SIZE,KERNEL_SIZE)) \n",
    "        self.conv3 = nn.Conv3d(32, 32, (KERNEL_SIZE,KERNEL_SIZE,KERNEL_SIZE)) \n",
    "        self.fc = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu( self.conv1(x) )\n",
    "        out = F.relu( self.conv2(out) )\n",
    "        out = F.relu( self.conv3(out) )\n",
    "        out = F.adaptive_max_pool3d(out, (1,1,1))\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # y = scaling(y).unsqueeze(1)\n",
    "        y = y.unsqueeze(1)\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # y = scaling(y).unsqueeze(1)\n",
    "        y = y.unsqueeze(1)\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"val_loss\", val_loss, on_epoch=True, prog_bar=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # y = scaling(y).unsqueeze(1)\n",
    "        y = y.unsqueeze(1)\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "# Check number of trainable parameters\n",
    "model = ShallowCNN()\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | conv1 | Conv3d | 2.6 K \n",
      "1 | conv2 | Conv3d | 27.7 K\n",
      "2 | conv3 | Conv3d | 27.7 K\n",
      "3 | fc    | Linear | 33    \n",
      "---------------------------------\n",
      "58.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "58.0 K    Total params\n",
      "0.232     Total estimated model params size (MB)\n",
      "Epoch 0:   5%|▌         | 30/563 [00:04<01:12,  7.34it/s, loss=1.54e+03, v_num=19, val_loss=4.02e+3, train_loss_step=694.0]\n",
      "/home/fiqah_kmy99_gmail_com/miniconda3/envs/gvp/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|██████████| 63/63 [00:02<00:00, 21.49it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 499.7353820800781}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_loss': 499.7353820800781}]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "task = 1\n",
    "\n",
    "dm = CNNDataModule(task=task)\n",
    "model = ShallowCNN()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=40)\n",
    "trainer.fit(model, dm)\n",
    "trainer.test(datamodule=dm)\n",
    "# To see the plots, on the terminal run: tensorboard --logdir ./lightning_logs\n"
   ]
  },
  {
   "source": [
    "# Resume training from checkpoint"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "n_loss_step=41.90, train_loss_epoch=42.30]\n",
      "Epoch 94:  89%|████████▉ | 501/563 [00:57<00:07,  8.78it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 94:  90%|████████▉ | 504/563 [00:57<00:06,  8.79it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  90%|█████████ | 507/563 [00:57<00:06,  8.82it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  91%|█████████ | 510/563 [00:57<00:05,  8.85it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  91%|█████████ | 513/563 [00:57<00:05,  8.88it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  92%|█████████▏| 516/563 [00:57<00:05,  8.92it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  92%|█████████▏| 519/563 [00:57<00:04,  8.95it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  93%|█████████▎| 522/563 [00:58<00:04,  8.99it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  93%|█████████▎| 525/563 [00:58<00:04,  9.02it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  94%|█████████▍| 528/563 [00:58<00:03,  9.05it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  94%|█████████▍| 531/563 [00:58<00:03,  9.08it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  95%|█████████▍| 534/563 [00:58<00:03,  9.12it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  95%|█████████▌| 537/563 [00:58<00:02,  9.15it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  96%|█████████▌| 540/563 [00:58<00:02,  9.18it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  96%|█████████▋| 543/563 [00:58<00:02,  9.22it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  97%|█████████▋| 546/563 [00:59<00:01,  9.25it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  98%|█████████▊| 549/563 [00:59<00:01,  9.27it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  98%|█████████▊| 552/563 [00:59<00:01,  9.30it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  99%|█████████▊| 555/563 [00:59<00:00,  9.33it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94:  99%|█████████▉| 558/563 [00:59<00:00,  9.35it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94: 100%|█████████▉| 561/563 [00:59<00:00,  9.39it/s, loss=60.2, v_num=19, val_loss=80.10, train_loss_step=42.10, train_loss_epoch=42.30]\n",
      "Epoch 94: 100%|██████████| 563/563 [00:59<00:00,  9.40it/s, loss=60.2, v_num=19, val_loss=86.20, train_loss_step=38.70, train_loss_epoch=43.40]\n",
      "Epoch 95:  89%|████████▉ | 501/563 [00:56<00:07,  8.81it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 95:  90%|████████▉ | 504/563 [00:57<00:06,  8.81it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Validating:   6%|▋         | 4/63 [00:00<00:04, 12.35it/s]\u001b[A\n",
      "Epoch 95:  90%|█████████ | 507/563 [00:57<00:06,  8.84it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  91%|█████████ | 510/563 [00:57<00:05,  8.87it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  91%|█████████ | 513/563 [00:57<00:05,  8.91it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  92%|█████████▏| 516/563 [00:57<00:05,  8.94it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  92%|█████████▏| 519/563 [00:57<00:04,  8.97it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  93%|█████████▎| 522/563 [00:58<00:04,  9.00it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  93%|█████████▎| 525/563 [00:58<00:04,  9.03it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  94%|█████████▍| 528/563 [00:58<00:03,  9.05it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  94%|█████████▍| 531/563 [00:58<00:03,  9.09it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  95%|█████████▍| 534/563 [00:58<00:03,  9.11it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  95%|█████████▌| 537/563 [00:58<00:02,  9.15it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  96%|█████████▌| 540/563 [00:58<00:02,  9.18it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  96%|█████████▋| 543/563 [00:58<00:02,  9.21it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  97%|█████████▋| 546/563 [00:59<00:01,  9.24it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  98%|█████████▊| 549/563 [00:59<00:01,  9.28it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  98%|█████████▊| 552/563 [00:59<00:01,  9.31it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  99%|█████████▊| 555/563 [00:59<00:00,  9.34it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95:  99%|█████████▉| 558/563 [00:59<00:00,  9.37it/s, loss=59, v_num=19, val_loss=86.20, train_loss_step=40.40, train_loss_epoch=43.40]\n",
      "Epoch 95: 100%|██████████| 563/563 [00:59<00:00,  9.41it/s, loss=59, v_num=19, val_loss=96.90, train_loss_step=39.90, train_loss_epoch=42.70]\n",
      "Epoch 96:  89%|████████▉ | 501/563 [00:56<00:07,  8.82it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/63 [00:00<00:15,  3.88it/s]\u001b[A\n",
      "Epoch 96:  90%|████████▉ | 504/563 [00:57<00:06,  8.82it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  90%|█████████ | 507/563 [00:57<00:06,  8.85it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Validating:  11%|█         | 7/63 [00:00<00:03, 14.73it/s]\u001b[A\n",
      "Epoch 96:  91%|█████████ | 510/563 [00:57<00:05,  8.87it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  91%|█████████ | 513/563 [00:57<00:05,  8.90it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  92%|█████████▏| 516/563 [00:57<00:05,  8.93it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  92%|█████████▏| 519/563 [00:57<00:04,  8.96it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  93%|█████████▎| 522/563 [00:58<00:04,  9.00it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  93%|█████████▎| 525/563 [00:58<00:04,  9.03it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  94%|█████████▍| 528/563 [00:58<00:03,  9.07it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  94%|█████████▍| 531/563 [00:58<00:03,  9.10it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  95%|█████████▍| 534/563 [00:58<00:03,  9.13it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  95%|█████████▌| 537/563 [00:58<00:02,  9.17it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  96%|█████████▌| 540/563 [00:58<00:02,  9.20it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  96%|█████████▋| 543/563 [00:58<00:02,  9.23it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  97%|█████████▋| 546/563 [00:58<00:01,  9.26it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  98%|█████████▊| 549/563 [00:59<00:01,  9.30it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  98%|█████████▊| 552/563 [00:59<00:01,  9.33it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  99%|█████████▊| 555/563 [00:59<00:00,  9.36it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96:  99%|█████████▉| 558/563 [00:59<00:00,  9.40it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96: 100%|█████████▉| 561/563 [00:59<00:00,  9.43it/s, loss=56.3, v_num=19, val_loss=96.90, train_loss_step=38.20, train_loss_epoch=42.70]\n",
      "Epoch 96: 100%|██████████| 563/563 [00:59<00:00,  9.44it/s, loss=56.3, v_num=19, val_loss=102.0, train_loss_step=39.30, train_loss_epoch=41.40]\n",
      "Epoch 97:  89%|████████▉ | 501/563 [00:56<00:07,  8.82it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/63 [00:00<00:13,  4.50it/s]\u001b[A\n",
      "Epoch 97:  90%|████████▉ | 504/563 [00:57<00:06,  8.82it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  90%|█████████ | 507/563 [00:57<00:06,  8.84it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  91%|█████████ | 510/563 [00:57<00:05,  8.87it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  91%|█████████ | 513/563 [00:57<00:05,  8.90it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  92%|█████████▏| 516/563 [00:57<00:05,  8.93it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  92%|█████████▏| 519/563 [00:57<00:04,  8.96it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  93%|█████████▎| 522/563 [00:58<00:04,  9.00it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  93%|█████████▎| 525/563 [00:58<00:04,  9.03it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  94%|█████████▍| 528/563 [00:58<00:03,  9.06it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  94%|█████████▍| 531/563 [00:58<00:03,  9.09it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  95%|█████████▍| 534/563 [00:58<00:03,  9.13it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  95%|█████████▌| 537/563 [00:58<00:02,  9.16it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  96%|█████████▌| 540/563 [00:58<00:02,  9.19it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  96%|█████████▋| 543/563 [00:58<00:02,  9.22it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  97%|█████████▋| 546/563 [00:59<00:01,  9.24it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  98%|█████████▊| 549/563 [00:59<00:01,  9.28it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  98%|█████████▊| 552/563 [00:59<00:01,  9.31it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  99%|█████████▊| 555/563 [00:59<00:00,  9.34it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97:  99%|█████████▉| 558/563 [00:59<00:00,  9.38it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97: 100%|█████████▉| 561/563 [00:59<00:00,  9.41it/s, loss=55, v_num=19, val_loss=102.0, train_loss_step=36.60, train_loss_epoch=41.40]\n",
      "Epoch 97: 100%|██████████| 563/563 [00:59<00:00,  9.42it/s, loss=55, v_num=19, val_loss=104.0, train_loss_step=38.10, train_loss_epoch=40.30]\n",
      "Epoch 98:  89%|████████▉ | 501/563 [00:57<00:07,  8.74it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/63 [00:00<00:16,  3.87it/s]\u001b[A\n",
      "Epoch 98:  90%|████████▉ | 504/563 [00:57<00:06,  8.73it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  90%|█████████ | 507/563 [00:57<00:06,  8.76it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  91%|█████████ | 510/563 [00:57<00:06,  8.79it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  91%|█████████ | 513/563 [00:58<00:05,  8.83it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  92%|█████████▏| 516/563 [00:58<00:05,  8.86it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  92%|█████████▏| 519/563 [00:58<00:04,  8.90it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  93%|█████████▎| 522/563 [00:58<00:04,  8.93it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  93%|█████████▎| 525/563 [00:58<00:04,  8.96it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  94%|█████████▍| 528/563 [00:58<00:03,  9.00it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  94%|█████████▍| 531/563 [00:58<00:03,  9.03it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  95%|█████████▍| 534/563 [00:58<00:03,  9.06it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  95%|█████████▌| 537/563 [00:59<00:02,  9.10it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  96%|█████████▌| 540/563 [00:59<00:02,  9.13it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  96%|█████████▋| 543/563 [00:59<00:02,  9.16it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  97%|█████████▋| 546/563 [00:59<00:01,  9.19it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  98%|█████████▊| 549/563 [00:59<00:01,  9.23it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  98%|█████████▊| 552/563 [00:59<00:01,  9.26it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  99%|█████████▊| 555/563 [00:59<00:00,  9.29it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98:  99%|█████████▉| 558/563 [00:59<00:00,  9.32it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98: 100%|█████████▉| 561/563 [00:59<00:00,  9.36it/s, loss=52.2, v_num=19, val_loss=104.0, train_loss_step=35.30, train_loss_epoch=40.30]\n",
      "Epoch 98: 100%|██████████| 563/563 [01:00<00:00,  9.37it/s, loss=52.2, v_num=19, val_loss=102.0, train_loss_step=38.40, train_loss_epoch=39.20]\n",
      "Epoch 99:  89%|████████▉ | 501/563 [00:56<00:07,  8.80it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/63 [00:00<00:14,  4.17it/s]\u001b[A\n",
      "Epoch 99:  90%|████████▉ | 504/563 [00:57<00:06,  8.80it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  90%|█████████ | 507/563 [00:57<00:06,  8.83it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  91%|█████████ | 510/563 [00:57<00:05,  8.87it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  91%|█████████ | 513/563 [00:57<00:05,  8.90it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  92%|█████████▏| 516/563 [00:57<00:05,  8.93it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  92%|█████████▏| 519/563 [00:57<00:04,  8.96it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  93%|█████████▎| 522/563 [00:58<00:04,  8.98it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  93%|█████████▎| 525/563 [00:58<00:04,  9.01it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  94%|█████████▍| 528/563 [00:58<00:03,  9.05it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  94%|█████████▍| 531/563 [00:58<00:03,  9.08it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  95%|█████████▍| 534/563 [00:58<00:03,  9.11it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  95%|█████████▌| 537/563 [00:58<00:02,  9.15it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  96%|█████████▌| 540/563 [00:58<00:02,  9.18it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  96%|█████████▋| 543/563 [00:58<00:02,  9.21it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  97%|█████████▋| 546/563 [00:59<00:01,  9.24it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  98%|█████████▊| 549/563 [00:59<00:01,  9.27it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  98%|█████████▊| 552/563 [00:59<00:01,  9.30it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  99%|█████████▊| 555/563 [00:59<00:00,  9.33it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99:  99%|█████████▉| 558/563 [00:59<00:00,  9.36it/s, loss=50.6, v_num=19, val_loss=102.0, train_loss_step=33.10, train_loss_epoch=39.20]\n",
      "Epoch 99: 100%|██████████| 563/563 [00:59<00:00,  9.41it/s, loss=50.6, v_num=19, val_loss=103.0, train_loss_step=35.90, train_loss_epoch=38.40]\n",
      "Epoch 99: 100%|██████████| 563/563 [00:59<00:00,  9.41it/s, loss=50.6, v_num=19, val_loss=103.0, train_loss_step=35.90, train_loss_epoch=38.40]\n",
      "Testing: 100%|██████████| 63/63 [00:02<00:00, 24.76it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 63.581539154052734}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_loss': 63.581539154052734}]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "task = 1\n",
    "\n",
    "dm = CNNDataModule(task=task)\n",
    "model = ShallowCNN()\n",
    "\n",
    "trainer = pl.Trainer(resume_from_checkpoint=\"./lightning_logs/version_18/checkpoints/epoch=39-step=19999.ckpt\", max_epochs=100)\n",
    "trainer.fit(model, dm)\n",
    "trainer.test(datamodule=dm)"
   ]
  },
  {
   "source": [
    "# Loading from checkpoint"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | conv1 | Conv3d | 49.2 K\n",
      "1 | fc    | Linear | 33    \n",
      "---------------------------------\n",
      "49.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "49.2 K    Total params\n",
      "0.197     Total estimated model params size (MB)\n",
      "Epoch 0:  89%|████████▉ | 500/563 [00:22<00:02, 22.67it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  89%|████████▉ | 502/563 [00:22<00:02, 22.52it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Epoch 0:  90%|█████████ | 507/563 [00:22<00:02, 22.63it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Epoch 0:  91%|█████████ | 513/563 [00:22<00:02, 22.78it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Epoch 0:  92%|█████████▏| 519/563 [00:22<00:01, 22.93it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Epoch 0:  93%|█████████▎| 525/563 [00:22<00:01, 23.08it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Epoch 0:  94%|█████████▍| 531/563 [00:22<00:01, 23.23it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Epoch 0:  95%|█████████▌| 537/563 [00:23<00:01, 23.33it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Epoch 0:  96%|█████████▋| 543/563 [00:23<00:00, 23.47it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Epoch 0:  98%|█████████▊| 549/563 [00:23<00:00, 23.60it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Epoch 0:  99%|█████████▊| 555/563 [00:23<00:00, 23.72it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Epoch 0: 100%|█████████▉| 561/563 [00:23<00:00, 23.86it/s, loss=167, v_num=14, val_loss=153.0, train_loss_step=114.0]\n",
      "Epoch 0: 100%|██████████| 563/563 [00:23<00:00, 23.88it/s, loss=167, v_num=14, val_loss=157.0, train_loss_step=158.0, train_loss_epoch=159.0]\n",
      "Epoch 1:  89%|████████▉ | 500/563 [00:21<00:02, 23.20it/s, loss=157, v_num=14, val_loss=157.0, train_loss_step=108.0, train_loss_epoch=159.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  90%|████████▉ | 504/563 [00:21<00:02, 23.06it/s, loss=157, v_num=14, val_loss=157.0, train_loss_step=108.0, train_loss_epoch=159.0]\n",
      "Epoch 1:  91%|█████████ | 510/563 [00:21<00:02, 23.20it/s, loss=157, v_num=14, val_loss=157.0, train_loss_step=108.0, train_loss_epoch=159.0]\n",
      "Validating:  16%|█▌        | 10/63 [00:00<00:01, 28.89it/s]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 516/563 [00:22<00:02, 23.30it/s, loss=157, v_num=14, val_loss=157.0, train_loss_step=108.0, train_loss_epoch=159.0]\n",
      "Epoch 1:  93%|█████████▎| 522/563 [00:22<00:01, 23.45it/s, loss=157, v_num=14, val_loss=157.0, train_loss_step=108.0, train_loss_epoch=159.0]\n",
      "Epoch 1:  94%|█████████▍| 528/563 [00:22<00:01, 23.54it/s, loss=157, v_num=14, val_loss=157.0, train_loss_step=108.0, train_loss_epoch=159.0]\n",
      "Epoch 1:  95%|█████████▍| 534/563 [00:22<00:01, 23.69it/s, loss=157, v_num=14, val_loss=157.0, train_loss_step=108.0, train_loss_epoch=159.0]\n",
      "Epoch 1:  96%|█████████▌| 540/563 [00:22<00:00, 23.83it/s, loss=157, v_num=14, val_loss=157.0, train_loss_step=108.0, train_loss_epoch=159.0]\n",
      "Epoch 1:  97%|█████████▋| 546/563 [00:22<00:00, 23.98it/s, loss=157, v_num=14, val_loss=157.0, train_loss_step=108.0, train_loss_epoch=159.0]\n",
      "Epoch 1:  98%|█████████▊| 552/563 [00:22<00:00, 24.13it/s, loss=157, v_num=14, val_loss=157.0, train_loss_step=108.0, train_loss_epoch=159.0]\n",
      "Epoch 1:  99%|█████████▉| 558/563 [00:22<00:00, 24.28it/s, loss=157, v_num=14, val_loss=157.0, train_loss_step=108.0, train_loss_epoch=159.0]\n",
      "Epoch 1: 100%|██████████| 563/563 [00:23<00:00, 24.38it/s, loss=157, v_num=14, val_loss=151.0, train_loss_step=146.0, train_loss_epoch=150.0]\n",
      "Epoch 2:  89%|████████▉ | 500/563 [00:21<00:02, 23.38it/s, loss=147, v_num=14, val_loss=151.0, train_loss_step=101.0, train_loss_epoch=150.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  90%|████████▉ | 504/563 [00:21<00:02, 23.24it/s, loss=147, v_num=14, val_loss=151.0, train_loss_step=101.0, train_loss_epoch=150.0]\n",
      "Epoch 2:  91%|█████████ | 510/563 [00:21<00:02, 23.40it/s, loss=147, v_num=14, val_loss=151.0, train_loss_step=101.0, train_loss_epoch=150.0]\n",
      "Epoch 2:  92%|█████████▏| 516/563 [00:21<00:01, 23.52it/s, loss=147, v_num=14, val_loss=151.0, train_loss_step=101.0, train_loss_epoch=150.0]\n",
      "Epoch 2:  93%|█████████▎| 522/563 [00:22<00:01, 23.63it/s, loss=147, v_num=14, val_loss=151.0, train_loss_step=101.0, train_loss_epoch=150.0]\n",
      "Validating:  35%|███▍      | 22/63 [00:00<00:01, 37.42it/s]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 528/563 [00:22<00:01, 23.76it/s, loss=147, v_num=14, val_loss=151.0, train_loss_step=101.0, train_loss_epoch=150.0]\n",
      "Epoch 2:  95%|█████████▍| 534/563 [00:22<00:01, 23.89it/s, loss=147, v_num=14, val_loss=151.0, train_loss_step=101.0, train_loss_epoch=150.0]\n",
      "Epoch 2:  96%|█████████▌| 540/563 [00:22<00:00, 24.03it/s, loss=147, v_num=14, val_loss=151.0, train_loss_step=101.0, train_loss_epoch=150.0]\n",
      "Epoch 2:  97%|█████████▋| 546/563 [00:22<00:00, 24.18it/s, loss=147, v_num=14, val_loss=151.0, train_loss_step=101.0, train_loss_epoch=150.0]\n",
      "Epoch 2:  98%|█████████▊| 552/563 [00:22<00:00, 24.32it/s, loss=147, v_num=14, val_loss=151.0, train_loss_step=101.0, train_loss_epoch=150.0]\n",
      "Epoch 2:  99%|█████████▉| 558/563 [00:22<00:00, 24.46it/s, loss=147, v_num=14, val_loss=151.0, train_loss_step=101.0, train_loss_epoch=150.0]\n",
      "Epoch 2: 100%|██████████| 563/563 [00:22<00:00, 24.55it/s, loss=147, v_num=14, val_loss=145.0, train_loss_step=136.0, train_loss_epoch=142.0]\n",
      "Epoch 3:  89%|████████▉ | 500/563 [00:21<00:02, 23.53it/s, loss=138, v_num=14, val_loss=145.0, train_loss_step=94.20, train_loss_epoch=142.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  90%|████████▉ | 504/563 [00:21<00:02, 23.40it/s, loss=138, v_num=14, val_loss=145.0, train_loss_step=94.20, train_loss_epoch=142.0]\n",
      "Epoch 3:  91%|█████████ | 510/563 [00:21<00:02, 23.53it/s, loss=138, v_num=14, val_loss=145.0, train_loss_step=94.20, train_loss_epoch=142.0]\n",
      "Epoch 3:  92%|█████████▏| 516/563 [00:21<00:01, 23.69it/s, loss=138, v_num=14, val_loss=145.0, train_loss_step=94.20, train_loss_epoch=142.0]\n",
      "Epoch 3:  93%|█████████▎| 522/563 [00:21<00:01, 23.83it/s, loss=138, v_num=14, val_loss=145.0, train_loss_step=94.20, train_loss_epoch=142.0]\n",
      "Validating:  35%|███▍      | 22/63 [00:00<00:00, 41.51it/s]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 528/563 [00:22<00:01, 23.94it/s, loss=138, v_num=14, val_loss=145.0, train_loss_step=94.20, train_loss_epoch=142.0]\n",
      "Epoch 3:  95%|█████████▍| 534/563 [00:22<00:01, 24.09it/s, loss=138, v_num=14, val_loss=145.0, train_loss_step=94.20, train_loss_epoch=142.0]\n",
      "Epoch 3:  96%|█████████▌| 540/563 [00:22<00:00, 24.23it/s, loss=138, v_num=14, val_loss=145.0, train_loss_step=94.20, train_loss_epoch=142.0]\n",
      "Epoch 3:  97%|█████████▋| 546/563 [00:22<00:00, 24.38it/s, loss=138, v_num=14, val_loss=145.0, train_loss_step=94.20, train_loss_epoch=142.0]\n",
      "Epoch 3:  98%|█████████▊| 552/563 [00:22<00:00, 24.53it/s, loss=138, v_num=14, val_loss=145.0, train_loss_step=94.20, train_loss_epoch=142.0]\n",
      "Epoch 3: 100%|██████████| 563/563 [00:22<00:00, 24.77it/s, loss=138, v_num=14, val_loss=138.0, train_loss_step=124.0, train_loss_epoch=133.0]\n",
      "Epoch 4:  55%|█████▍    | 308/563 [00:13<00:11, 22.74it/s, loss=128, v_num=14, val_loss=138.0, train_loss_step=127.0, train_loss_epoch=133.0]\n",
      "/home/fiqah_kmy99_gmail_com/miniconda3/envs/gvp/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|██████████| 63/63 [00:01<00:00, 40.11it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 4077.56591796875}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_loss': 4077.56591796875}]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "task = 1\n",
    "\n",
    "dm = CNNDataModule(task=task)\n",
    "model = ShallowCNN.load_from_checkpoint(\"./lightning_logs/version_13/checkpoints/epoch=14-step=7499.ckpt\")\n",
    "model.eval()\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "trainer.test(model, datamodule=dm)\n",
    "\n"
   ]
  }
 ]
}